name: Scrape PaknSave (manual)

on:
  workflow_dispatch:
    inputs:
      department:
        description: "Department to scrape (e.g., pantry)"
        required: false
        default: pantry
      max_pages:
        description: "Max pages per department"
        required: false
        default: "3"

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install minimal deps
        working-directory: backend/ocr
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-scraper.txt

      - name: Run Cloudflare-aware PaknSave scraper (test only)
        working-directory: backend/ocr
        env:
          HTTP_PROXIES: ${{ secrets.HTTP_PROXIES }}
          SCRAPER_PROXIES: ${{ secrets.SCRAPER_PROXIES }}
        run: |
          python cloudflare_scraper.py

      # Note: Production inserts for PaknSave require robust Cloudflare handling
      # and are not enabled by default. This workflow is intended for manual testing
      # with proxies to validate access and product parsing.

